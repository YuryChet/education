{"cells":[{"cell_type":"markdown","metadata":{"id":"BxCPH8VeKcdg"},"source":["# Домашнее задание 3\n","\n","В этом задании напишем простое решение классификации датасета `FashionMNIST`, а затем будем его улучшать с помощью:\n","- dropout;\n","- batch normalization;\n","- LR scheduler;\n","\n","В конце сохраним модель в файл и убедимся, что этот файл можем в дальнейшем прочитать."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xARvjMxP4JWd","executionInfo":{"status":"ok","timestamp":1742053371249,"user_tz":-180,"elapsed":26132,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e6c8b5a-c312-4419-82d9-1083bd415d2a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7ad956885170>"]},"metadata":{},"execution_count":1}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.datasets import FashionMNIST\n","from torchvision.transforms import ToTensor\n","from dataclasses import dataclass\n","\n","torch.manual_seed(987)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fmMiGhR4SDH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742053376881,"user_tz":-180,"elapsed":5574,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"c75b9a28-289c-45e1-bb38-ee9f54a1fb90"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:01<00:00, 15.8MB/s]\n","100%|██████████| 29.5k/29.5k [00:00<00:00, 270kB/s]\n","100%|██████████| 4.42M/4.42M [00:00<00:00, 5.06MB/s]\n","100%|██████████| 5.15k/5.15k [00:00<00:00, 8.28MB/s]\n"]}],"source":["# Загружаем датасет с картинками одежды FashionMNIST\n","# root- путь\n","# train- трейновая часть\n","# download- если отсутствует то загрузить?\n","# transofrm преобразуем сразу в тензор\n","\n","train_dataset = FashionMNIST(\n","    root=\"./data\", train=True, download=True, transform=ToTensor()\n",")\n","test_dataset = FashionMNIST(\n","    root=\"./data\", train=False, download=True, transform=ToTensor()\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"egta3KBe4fkg"},"outputs":[],"source":["# тут дело в том, что в датасете есть\n","# интересный момент, в предыущем шаге мы создавали датасет train_dataset с ToTensor\n","# тут мы обращаемся к этому объекту через .data к его неоработанному массиву (не теноры)\n","X_train = train_dataset.data.float()\n","y_train = train_dataset.targets\n","X_test = test_dataset.data.float()\n","y_test = test_dataset.targets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PvVbvSjN5KnY"},"outputs":[],"source":["@dataclass\n","class TrainConfig:\n","    lr: float = 1e-4 # было 3, увеличил в 5-10 раз\n","    total_iterations: int = 100\n","\n","\n","# Для оценки будем использовать метрику accuracy\n","# Подумайте (опционально), какие еще метрики можно использовать\n","# на вход функция принимает y_pred и y_true\n","def calculate_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n","    _, predicted = torch.max(y_pred, 1) # y_pred выдает матрицу с вероятностями у каждого класса. Значение в строчке и есть вероятности. Строчек может быть много\n","    # первую переменную(сами значения) пропускает, записывате только вторую(индексы этих значений). после = ищет максимльное значение в y_pred по СТРОЧКАМ (так как 1)(было бы 0 были было по столбцам)\n","    correct = (predicted == y_true).float().sum() # проверка что конкретные значения равны y_true. В тип флоат и суммах их\n","    accuracy = correct / y_true.shape[0]\n","    return accuracy.item()"]},{"cell_type":"markdown","metadata":{"id":"lXc2c8a6Kcdq"},"source":["## Задание №1\n","\n","Попробуйте реализовать простой бейзлайн с несколькими слоями:\n","- Linear\n","- ReLU\n","- Linear\n","\n","Вставьте свою релизацию `SimpleModel` в проверку.\n","Вам нужно дописать и сдать как `SimpleModel`, так и `train_loop`.\n","\n","Используйте кросс-энтропию как функцию потерь."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kect6sBKKcdq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Orzur80V5J9j","colab":{"base_uri":"https://localhost:8080/","height":486},"executionInfo":{"status":"error","timestamp":1742053440527,"user_tz":-180,"elapsed":33214,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"f06ffb57-ed75-47e7-d7ed-5babee642198"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 10/100 [00:20<02:49,  1.89s/it]"]},{"output_type":"stream","name":"stdout","text":["tensor([[ -4.4852,  -7.5957,  -0.1859,  ...,  28.3525,   8.4731,  -6.3709],\n","        [ -2.0865,   0.0304,   4.6188,  ...,  -0.9735,  -3.1472,  -0.7510],\n","        [  1.9511,  -1.7994,  -0.9848,  ...,  14.0024,   2.6934,  29.5953],\n","        ...,\n","        [  3.0517,   2.2798,   6.7182,  ...,   4.9355,  -0.6009,  -2.9287],\n","        [ 24.6324,  -0.3651,   6.7563,  ...,  -7.6748,  14.8810, -25.5284],\n","        [  3.0767,  12.7606,  -3.4705,  ...,  -5.1555,  -5.8980,  -4.1368]],\n","       grad_fn=<AddmmBackward0>)\n","Iteration 10: Train Loss = 1.4895, Tra  in Acc = 0.6277, Val Loss = 1.4135, Val Acc = 0.6387\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 15/100 [00:30<02:54,  2.06s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-fb88d8b455c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Создаём объект!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Создаём объект TrainConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-5-fb88d8b455c9>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(model, X_train, y_train, X_val, y_val, config)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m# Forward pass: вычисляем предсказания модели\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# Вычисляем лосс\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-fb88d8b455c9>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","# разбиение на трейн тест\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","\n","# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class SimpleModel(nn.Module):\n","  def __init__(self, num_classes: int):\n","      super().__init__()\n","\n","\n","      # размерность после первого слоя\n","      hidden_dim = 512\n","\n","      self.net = nn.Sequential(\n","          nn.Linear(in_features= 28*28 , out_features= hidden_dim),\n","          nn.ReLU(),\n","          nn.Linear(in_features=hidden_dim, out_features=num_classes)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","        # входной вектор мы решейпим до тензора и этих тензоров столько сколько картинок\n","        # так как это подается в первый слой, он должен иметь такую же размерность\n","        x = x.reshape((-1, 28 * 28))\n","\n","        return self.net(x)\n","\n","\n","\n","  # \"\"\"Обучите здесь модель, подсчитайте метрики на валидационной выборке.\n","\n","  #   Можете так же писать/рисовать accuracy в процессе обучения.\n","  #   Например, каждые 10 итераций или даже каждую итерацию.\n","  #   \"\"\"\n","\n","\n","\n","\n","\n","\n","# model.parameters() возвращает все обучаемые параметры модели (в виде тензоров).\n","# Откуда функция все эти параметры узнает, ведь мы нигде их явно не прописывали в классе модели?\n","# Это магия PyTorch, он умеет многое делать сам :)\n","\n","\n","\n","def train_loop(\n","    model: SimpleModel,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    # создается оптимайзер\n","    optimizer = optim.SGD(params=model.parameters(), lr= config.lr)\n","\n","    losses = []\n","    loss_fn = nn.CrossEntropyLoss()\n","    # Пройдемся 2000 раз по всем данным\n","    for i in tqdm.trange(config.total_iterations):\n","        # optimizer по умолчанию \"помнит\" градиенты с прошлых итераций - и прибавляет к новым.\n","        # Так сделано ради продвинутых техник обучения.\n","        # Нам это не надо, поэтому в каждой итерации явно зануляем все градиенты - пусть считаются заново.\n","        optimizer.zero_grad()\n","\n","        # Это блок уже знаем: считаем выход, потери, градиенты по потерям.\n","        # Forward pass: вычисляем предсказания модели\n","        # 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\n","        outputs = model(X_train)\n","\n","        # Вычисляем лосс\n","        loss = loss_fn(outputs, y_train) # Убираем лишние размерности, если они есть\n","\n","        # Backward pass: вычисляем градиенты\n","        loss.backward()\n","\n","        # Просим оптимизатор пройтись по параметрам и сделать градиентный спуск.\n","        # Оптимизатор сам обновит веса, вручную этого делать не надо\n","        optimizer.step()\n","\n","        # Запомним loss\n","        losses.append(loss.detach().item())\n","\n","\n","        # ЦИКЛ ДЛЯ ОЦЕНКИ МОДЕЛИ\n","        if (i + 1) % 10 == 0: # проверяем что i кратен 10 (делится ли на 10 без остатка)\n","            model.eval() # переключаем на режим оценки inference mode\n","            with torch.no_grad(): # отключает подсчеты градиента для все вычислений внутри блока with\n","                train_acc = calculate_accuracy(outputs, y_train) # outputs-предсказания сравнивниваем с y_train\n","                val_outputs = model(X_val) # запускам валидационные данные в модель ?????? Это ещё не вероятности\n","                val_loss = F.cross_entropy(val_outputs, y_val)  #функция потерь. Сравниваем с y_val\n","                val_acc = calculate_accuracy(val_outputs, y_val) # считаем точность\n","            model.train() # включаем обратно BatchNorm и дропауты\n","            print(\n","                f\"Iteration {i+1}: Train Loss = {loss.item():.4f}, Tra  in Acc = {train_acc:.4f}, Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}\"\n","            )\n","\n","\n","\n","    return losses\n","\n","\n","model = SimpleModel(num_classes=10)  # Создаём объект!\n","config = TrainConfig()  # Создаём объект TrainConfig\n","losses = train_loop(model, X_train, y_train, X_val, y_val, config)"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"AvvHwk_4ER94","executionInfo":{"status":"error","timestamp":1742029793935,"user_tz":-180,"elapsed":650,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"218a2416-0da5-4a7c-bc3f-89befd2a16ab"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'outputs' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-0dde94a30ad7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"BIn3cOEwKcdq"},"source":["## Задание №2\n","Какое максимальное значение метрики accuracy удалось получить в процессе обучения?\n","\n","Округлите до 3 значений после запятой\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1O8NRkaAe1xw"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"083v_Y3tKcdr"},"source":["## Задание №3\n","Добавьте один `dropout` слой в вашу модель.\n","\n","_Подумайте, что может поменяться при перестановке ReLU и Dropout слоев местами._"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-0l7KJ5t6MDo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741952800203,"user_tz":-180,"elapsed":240845,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"2b22f6e9-7f08-4921-9da7-3bc535d216b7"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 10/100 [00:27<03:47,  2.53s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 10: Train Loss = 2.4228, Tra  in Acc = 0.6079, Val Loss = 1.3508, Val Acc = 0.7053\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 20/100 [00:51<03:15,  2.44s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 20: Train Loss = 1.6807, Tra  in Acc = 0.6489, Val Loss = 0.9928, Val Acc = 0.7375\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 30/100 [01:14<02:57,  2.54s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 30: Train Loss = 1.4055, Tra  in Acc = 0.6688, Val Loss = 0.8672, Val Acc = 0.7542\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [01:37<02:20,  2.34s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 40: Train Loss = 1.2445, Tra  in Acc = 0.6851, Val Loss = 0.7957, Val Acc = 0.7652\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 50/100 [02:01<01:54,  2.29s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 50: Train Loss = 1.1495, Tra  in Acc = 0.6932, Val Loss = 0.7493, Val Acc = 0.7702\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [02:25<01:32,  2.31s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 60: Train Loss = 1.0783, Tra  in Acc = 0.7054, Val Loss = 0.7162, Val Acc = 0.7790\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 70/100 [02:49<01:10,  2.36s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 70: Train Loss = 1.0348, Tra  in Acc = 0.7123, Val Loss = 0.6914, Val Acc = 0.7843\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [03:12<00:48,  2.42s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 80: Train Loss = 0.9758, Tra  in Acc = 0.7200, Val Loss = 0.6713, Val Acc = 0.7863\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 90/100 [03:36<00:25,  2.57s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 90: Train Loss = 0.9429, Tra  in Acc = 0.7257, Val Loss = 0.6548, Val Acc = 0.7895\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [04:00<00:00,  2.41s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 100: Train Loss = 0.9269, Tra  in Acc = 0.7290, Val Loss = 0.6410, Val Acc = 0.7913\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["## Возможно, класс нужно отнаследовать от некого класса из pytorch\n","# class DropoutModel:\n","#    hidden_dim = 512\n","\n","\n","# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()\n","\n","\n","# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class DropoutModel(nn.Module):\n","  def __init__(self, num_classes: int, dropout_rate: float = 0.5):\n","      super().__init__()\n","\n","\n","      # размерность после первого слоя\n","      hidden_dim = 512\n","\n","      self.net = nn.Sequential(\n","          nn.Linear(in_features= 28*28 , out_features= hidden_dim),\n","          nn.ReLU(),\n","          nn.Dropout(p=dropout_rate),\n","          nn.Linear(in_features=hidden_dim, out_features=num_classes)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","        # входной вектор мы решейпим до тензора и этих тензоров столько сколько картинок\n","        # так как это подается в первый слой, он должен иметь такую же размерность\n","        x = x.reshape((-1, 28 * 28))\n","\n","        return self.net(x)\n","\n","\n","\n","  # \"\"\"Обучите здесь модель, подсчитайте метрики на валидационной выборке.\n","\n","  #   Можете так же писать/рисовать accuracy в процессе обучения.\n","  #   Например, каждые 10 итераций или даже каждую итерацию.\n","  #   \"\"\"\n","\n","\n","\n","\n","\n","\n","# model.parameters() возвращает все обучаемые параметры модели (в виде тензоров).\n","# Откуда функция все эти параметры узнает, ведь мы нигде их явно не прописывали в классе модели?\n","# Это магия PyTorch, он умеет многое делать сам :)\n","\n","\n","\n","def train_loop(\n","    model: DropoutModel,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    # создается оптимайзер\n","    optimizer = optim.SGD(params=model.parameters(), lr= config.lr)\n","\n","    losses = []\n","    loss_fn = nn.CrossEntropyLoss()\n","    # Пройдемся 2000 раз по всем данным\n","    for i in tqdm.trange(config.total_iterations):\n","        # optimizer по умолчанию \"помнит\" градиенты с прошлых итераций - и прибавляет к новым.\n","        # Так сделано ради продвинутых техник обучения.\n","        # Нам это не надо, поэтому в каждой итерации явно зануляем все градиенты - пусть считаются заново.\n","        optimizer.zero_grad()\n","\n","        # Это блок уже знаем: считаем выход, потери, градиенты по потерям.\n","        # Forward pass: вычисляем предсказания модели\n","        # 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\n","        outputs = model(X_train)\n","\n","        # Вычисляем лосс\n","        loss = loss_fn(outputs, y_train) # Убираем лишние размерности, если они есть\n","\n","        # Backward pass: вычисляем градиенты\n","        loss.backward()\n","\n","        # Просим оптимизатор пройтись по параметрам и сделать градиентный спуск.\n","        # Оптимизатор сам обновит веса, вручную этого делать не надо\n","        optimizer.step()\n","\n","        # Запомним loss\n","        losses.append(loss.detach().item())\n","\n","\n","        # ЦИКЛ ДЛЯ ОЦЕНКИ МОДЕЛИ\n","        if (i + 1) % 10 == 0: # проверяем что i кратен 10 (делится ли на 10 без остатка)\n","            model.eval() # переключаем на режим оценки inference mode\n","            with torch.no_grad(): # отключает подсчеты градиента для все вычислений внутри блока with\n","                train_acc = calculate_accuracy(outputs, y_train) # outputs-предсказания сравнивниваем с y_train\n","                val_outputs = model(X_val) # запускам валидационные данные в модель ?????? Это ещё не вероятности\n","                val_loss = F.cross_entropy(val_outputs, y_val)  #функция потерь. Сравниваем с y_val\n","                val_acc = calculate_accuracy(val_outputs, y_val) # считаем точность\n","            model.train() # включаем обратно BatchNorm и дропауты\n","            print(\n","                f\"Iteration {i+1}: Train Loss = {loss.item():.4f}, Tra  in Acc = {train_acc:.4f}, Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}\"\n","            )\n","\n","\n","\n","    return losses\n","\n","\n","model = DropoutModel(num_classes=10, dropout_rate=0.5)  # Создаём объект!\n","config = TrainConfig()  # Создаём объект TrainConfig\n","losses = train_loop(model, X_train, y_train, X_val, y_val, config)"]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"gzo3ITRZyr6y","executionInfo":{"status":"error","timestamp":1741941302680,"user_tz":-180,"elapsed":203,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"78f68125-3c26-454c-d4af-3d98d00077fb"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'y_pred' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-3aaf935e6aec>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8HXca_LDEb-1"},"outputs":[],"source":["torch.manual_seed(987)\n","config = TrainConfig()\n","# Ваш код для обучения и подсчета accuracy"]},{"cell_type":"markdown","metadata":{"id":"eZ887jeFKcdr"},"source":["## Задание №4\n","Какое максимальное значение accuracy получилось в ходе обучения модели?\n","\n","Округлите до 3х знаков после запятой и отправьте в ЛМС."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sQoHAYnR91x-"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"Xdis_LAvLoNJ"},"source":["## Задание №5\n","\n","Добавьте `BatchNorm` в вашу модель.\n","Отправьте в ЛМС реализацию.\n","\n","Стоит ли делать BatchNorm до ReLU или после него?\n","Это дискуссионный вопрос, чаще всего применяют сначала нелинейность, затем Batch Norm.\n","Один из аргументов: при таком подходе данные на выходе будут иметь среднее 0 - что и ожидают люди, когда добавляют нормализацию.\n","\n","_[Дискуссия на Reddit](https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/)_\n","\n","Для определенности в этом задании будем следовать такому порядку: сначала ReLU, затем Batch Norm."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gm4LCPfW7wYJ"},"outputs":[],"source":["# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class BatchNormModel:\n","    hidden_dim = 512\n","    ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DaAkxSf-LsHp"},"outputs":[],"source":["torch.manual_seed(987)\n","config = TrainConfig()\n","# Ваш код для обучения и подсчета accuracy"]},{"cell_type":"code","source":["## Возможно, класс нужно отнаследовать от некого класса из pytorch\n","# class DropoutModel:\n","#    hidden_dim = 512\n","\n","\n","# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()\n","\n","\n","# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class BatchNormModel(nn.Module):\n","  def __init__(self, num_classes: int, dropout_rate: float = 0.5):\n","      super().__init__()\n","\n","\n","      # размерность после первого слоя\n","      hidden_dim = 512\n","\n","      self.net = nn.Sequential(\n","          nn.Linear(in_features= 28*28 , out_features= hidden_dim),\n","          nn.BatchNorm1d(num_features=hidden_dim),\n","          nn.ReLU(),\n","          nn.Dropout(p=dropout_rate),\n","          nn.Linear(in_features=hidden_dim, out_features=num_classes)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","        # входной вектор мы решейпим до тензора и этих тензоров столько сколько картинок\n","        # так как это подается в первый слой, он должен иметь такую же размерность\n","        x = x.reshape((-1, 28 * 28))\n","\n","        return self.net(x)\n","\n","\n","\n","  # \"\"\"Обучите здесь модель, подсчитайте метрики на валидационной выборке.\n","\n","  #   Можете так же писать/рисовать accuracy в процессе обучения.\n","  #   Например, каждые 10 итераций или даже каждую итерацию.\n","  #   \"\"\"\n","\n","\n","\n","\n","\n","\n","# model.parameters() возвращает все обучаемые параметры модели (в виде тензоров).\n","# Откуда функция все эти параметры узнает, ведь мы нигде их явно не прописывали в классе модели?\n","# Это магия PyTorch, он умеет многое делать сам :)\n","\n","\n","\n","def train_loop(\n","    model: DropoutModel,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    # создается оптимайзер\n","    optimizer = optim.SGD(params=model.parameters(), lr= config.lr)\n","\n","    losses = []\n","    loss_fn = nn.CrossEntropyLoss()\n","    # Пройдемся 2000 раз по всем данным\n","\n","\n","\n","    for i in tqdm.trange(config.total_iterations):\n","        # optimizer по умолчанию \"помнит\" градиенты с прошлых итераций - и прибавляет к новым.\n","        # Так сделано ради продвинутых техник обучения.\n","        # Нам это не надо, поэтому в каждой итерации явно зануляем все градиенты - пусть считаются заново.\n","        optimizer.zero_grad()\n","\n","        # Это блок уже знаем: считаем выход, потери, градиенты по потерям.\n","        # Forward pass: вычисляем предсказания модели\n","        # 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\n","        outputs = model(X_train)\n","\n","        # Вычисляем лосс\n","        loss = loss_fn(outputs, y_train) # Убираем лишние размерности, если они есть\n","\n","        # Backward pass: вычисляем градиенты\n","        loss.backward()\n","\n","        # Просим оптимизатор пройтись по параметрам и сделать градиентный спуск.\n","        # Оптимизатор сам обновит веса, вручную этого делать не надо\n","        optimizer.step()\n","\n","        # Запомним loss\n","        losses.append(loss.detach().item())\n","\n","\n","        # ЦИКЛ ДЛЯ ОЦЕНКИ МОДЕЛИ\n","        if (i + 1) % 10 == 0: # проверяем что i кратен 10 (делится ли на 10 без остатка)\n","            model.eval() # переключаем на режим оценки inference mode\n","            with torch.no_grad(): # отключает подсчеты градиента для все вычислений внутри блока with\n","                train_acc = calculate_accuracy(outputs, y_train) # outputs-предсказания сравнивниваем с y_train\n","                val_outputs = model(X_val) # запускам валидационные данные в модель ?????? Это ещё не вероятности\n","                val_loss = F.cross_entropy(val_outputs, y_val)  #функция потерь. Сравниваем с y_val\n","                val_acc = calculate_accuracy(val_outputs, y_val) # считаем точность\n","            model.train() # включаем обратно BatchNorm и дропауты\n","            print(\n","                f\"Iteration {i+1}: Train Loss = {loss.item():.4f}, Tra  in Acc = {train_acc:.4f}, Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}\"\n","            )\n","\n","\n","\n","    return losses\n","\n","\n","model = BatchNormModel(num_classes=10, dropout_rate=0.5)  # Создаём объект!\n","config = TrainConfig()  # Создаём объект TrainConfig\n","losses = train_loop(model, X_train, y_train, X_val, y_val, config)"],"metadata":{"id":"T-gI1CyYzJ_J"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQO-G6qqKcds"},"source":["## Задание №6\n","Какое максимальное значение `accuracy` получилось в ходе обучения модели?\n","\n","Округлите до 3х знаков после запятой."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"brmU-sorKcds"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"MhzJM6t_Kcdt"},"source":["Результат batch normalization мог не особо порадовать.\n","Но не спешите с выводами насчет этого слоя!\n","\n","Попробуйте обучить заново все три модели со значением `lr=1e-2` (в 10 раз больше).\n","Сравните результаты моделей и сделайте вывод."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GV74ZTR1Kcdt"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"aOcw53sMKcdt"},"source":["## Задание №7\n","Добавьте `LRscheduler` в вашу модель.\n","\n","Подробнее про `schedulers` можно почитать в [документации](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)"]},{"cell_type":"code","source":["## Возможно, класс нужно отнаследовать от некого класса из pytorch\n","# class DropoutModel:\n","#    hidden_dim = 512\n","\n","\n","# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","from torch.optim.lr_scheduler import StepLR\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()\n","\n","\n","# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class BatchNormModel(nn.Module):\n","  def __init__(self, num_classes: int, dropout_rate: float = 0.5):\n","      super().__init__()\n","\n","\n","      # размерность после первого слоя\n","      hidden_dim = 512\n","\n","      self.net = nn.Sequential(\n","          nn.Linear(in_features= 28*28 , out_features= hidden_dim),\n","          nn.BatchNorm1d(num_features=hidden_dim),\n","          nn.ReLU(),\n","          nn.Dropout(p=dropout_rate),\n","          nn.Linear(in_features=hidden_dim, out_features=num_classes)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","        # входной вектор мы решейпим до тензора и этих тензоров столько сколько картинок\n","        # так как это подается в первый слой, он должен иметь такую же размерность\n","        x = x.reshape((-1, 28 * 28))\n","\n","        return self.net(x)\n","\n","\n","\n","  # \"\"\"Обучите здесь модель, подсчитайте метрики на валидационной выборке.\n","\n","  #   Можете так же писать/рисовать accuracy в процессе обучения.\n","  #   Например, каждые 10 итераций или даже каждую итерацию.\n","  #   \"\"\"\n","\n","\n","\n","\n","\n","\n","# model.parameters() возвращает все обучаемые параметры модели (в виде тензоров).\n","# Откуда функция все эти параметры узнает, ведь мы нигде их явно не прописывали в классе модели?\n","# Это магия PyTorch, он умеет многое делать сам :)\n","\n","\n","\n","def train_loop_with_scheduler(\n","    model: BatchNormModel,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    # создается оптимайзер\n","    optimizer = optim.SGD(params=model.parameters(), lr= config.lr)\n","\n","    losses = []\n","    loss_fn = nn.CrossEntropyLoss()\n","    # Пройдемся 2000 раз по всем данным\n","\n","      #### Новое: создаем разный ШЕДУЛЕР в зависимости от конфига ####\n","    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n","\n","\n","\n","\n","\n","    for i in tqdm.trange(config.total_iterations):\n","        # optimizer по умолчанию \"помнит\" градиенты с прошлых итераций - и прибавляет к новым.\n","        # Так сделано ради продвинутых техник обучения.\n","        # Нам это не надо, поэтому в каждой итерации явно зануляем все градиенты - пусть считаются заново.\n","        optimizer.zero_grad()\n","\n","        # Это блок уже знаем: считаем выход, потери, градиенты по потерям.\n","        # Forward pass: вычисляем предсказания модели\n","        # 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\n","        outputs = model(X_train)\n","\n","        # Вычисляем лосс\n","        loss = loss_fn(outputs, y_train) # Убираем лишние размерности, если они есть\n","\n","        # Backward pass: вычисляем градиенты\n","        loss.backward()\n","\n","        # Просим оптимизатор пройтись по параметрам и сделать градиентный спуск.\n","        # Оптимизатор сам обновит веса, вручную этого делать не надо\n","        optimizer.step()\n","\n","\n","        # Запомним loss\n","        losses.append(loss.detach().item())\n","\n","\n","        # ЦИКЛ ДЛЯ ОЦЕНКИ МОДЕЛИ\n","        if (i + 1) % 10 == 0: # проверяем что i кратен 10 (делится ли на 10 без остатка)\n","            model.eval() # переключаем на режим оценки inference mode\n","            with torch.no_grad(): # отключает подсчеты градиента для все вычислений внутри блока with\n","                train_acc = calculate_accuracy(outputs, y_train) # outputs-предсказания сравнивниваем с y_train\n","                val_outputs = model(X_val) # запускам валидационные данные в модель ?????? Это ещё не вероятности\n","                val_loss = F.cross_entropy(val_outputs, y_val)  #функция потерь. Сравниваем с y_val\n","                val_acc = calculate_accuracy(val_outputs, y_val) # считаем точность\n","            model.train() # включаем обратно BatchNorm и дропауты\n","            print(\n","                f\"Iteration {i+1}: Train Loss = {loss.item():.4f}, Tra  in Acc = {train_acc:.4f}, Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}\"\n","            )\n","\n","        scheduler.step()\n","\n","    return losses\n","\n","\n","model = BatchNormModel(num_classes=10, dropout_rate=0.5)  # Создаём объект!\n","config = TrainConfig()  # Создаём объект TrainConfig\n","losses = train_loop_with_scheduler(model, X_train, y_train, X_val, y_val, config)"],"metadata":{"id":"1AN1UTlyg1lr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742057114562,"user_tz":-180,"elapsed":197578,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"ccf4332d-ed40-4778-824b-074687b01e8f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 10/100 [00:21<03:10,  2.11s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 10: Train Loss = 2.3504, Tra  in Acc = 0.1232, Val Loss = 2.2887, Val Acc = 0.1385\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 20/100 [00:40<02:28,  1.85s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 20: Train Loss = 2.3471, Tra  in Acc = 0.1279, Val Loss = 2.2755, Val Acc = 0.1404\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 30/100 [01:01<02:21,  2.02s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 30: Train Loss = 2.3487, Tra  in Acc = 0.1271, Val Loss = 2.2752, Val Acc = 0.1436\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [01:20<01:52,  1.88s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 40: Train Loss = 2.3479, Tra  in Acc = 0.1269, Val Loss = 2.2754, Val Acc = 0.1424\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 50/100 [01:40<01:36,  1.92s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 50: Train Loss = 2.3486, Tra  in Acc = 0.1260, Val Loss = 2.2755, Val Acc = 0.1429\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [01:58<01:12,  1.82s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 60: Train Loss = 2.3450, Tra  in Acc = 0.1290, Val Loss = 2.2756, Val Acc = 0.1427\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 70/100 [02:19<00:58,  1.94s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 70: Train Loss = 2.3496, Tra  in Acc = 0.1256, Val Loss = 2.2756, Val Acc = 0.1427\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [02:38<00:39,  2.00s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 80: Train Loss = 2.3497, Tra  in Acc = 0.1273, Val Loss = 2.2756, Val Acc = 0.1427\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 90/100 [02:57<00:18,  1.90s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 90: Train Loss = 2.3516, Tra  in Acc = 0.1256, Val Loss = 2.2756, Val Acc = 0.1427\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [03:16<00:00,  1.97s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 100: Train Loss = 2.3492, Tra  in Acc = 0.1252, Val Loss = 2.2756, Val Acc = 0.1427\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6cMUEd9G-5Bi"},"outputs":[],"source":["from torch.optim.lr_scheduler import StepLR\n","\n","\n","def train_loop_with_scheduler(\n","    model,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    ...\n","    scheduler = StepLR(..., step_size=5, gamma=0.1)\n","    ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uf39c7d-BU1a"},"outputs":[],"source":["torch.manual_seed(987)\n","config = TrainConfig(lr=1e-3)\n","# Ваш код для обучения и подсчета accuracy"]},{"cell_type":"markdown","metadata":{"id":"ipg05iJoKcdt"},"source":["## Задание №8\n","\n","Поэксперементируйте с параметрами нейронной сети, попробуйте добиться максимальной метрики `accuracy`.\n","\n","- попробуйте комбинацию Drouput + Batch Normalization и подумайте, как лучше всего раскрыть силу batch normalization (вспомните эксперименты с lr);\n","- попробуйте подвигать вероятность в Dropout;\n","- ну, или подержите обучение подольше, поставив больше шагов :)\n","\n","В ЛМС нужно сдать код класса `ExpModel`.\n","Вам необходимо выбить accuracy > 80%, чтобы сдать этот пункт."]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.datasets import FashionMNIST\n","from torchvision.transforms import ToTensor\n","from dataclasses import dataclass\n","\n","torch.manual_seed(987)\n","\n","\n","\n","\n","\n","@dataclass\n","class TrainConfig:\n","    lr: float = 1e-1 # было 1е-3\n","    total_iterations: int = 100\n","\n","\n","# Для оценки будем использовать метрику accuracy\n","# Подумайте (опционально), какие еще метрики можно использовать\n","# на вход функция принимает y_pred и y_true\n","def calculate_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n","    _, predicted = torch.max(y_pred, 1) # первую переменную(конкретные значения) пропускает, записывате только вторую. после = ищет максимльное значение в y_pred по строчкам (так как 1)(было бы 0 были было по столбцам)\n","    correct = (predicted == y_true).float().sum() # проверка что конкретные значения равны y_true. В тип флоат и суммах их\n","    accuracy = correct / y_true.shape[0]\n","    return accuracy.item()\n","\n","\n","\n","\n","\n","\n","# Загружаем датасет с картинками одежды FashionMNIST\n","# root- путь\n","# train- трейновая часть\n","# download- если отсутствует то загрузить?\n","# transofrm преобразуем сразу в тензор\n","\n","train_dataset = FashionMNIST(\n","    root=\"./data\", train=True, download=True, transform=ToTensor()\n",")\n","test_dataset = FashionMNIST(\n","    root=\"./data\", train=False, download=True, transform=ToTensor()\n",")\n","\n","# тут дело в том, что в датасете есть\n","# интересный момент, в предыущем шаге мы создавали датасет train_dataset с ToTensor\n","# тут мы обращаемся к этому объекту через .data к его неоработанному массиву (не теноры)\n","X_train = train_dataset.data.float()\n","y_train = train_dataset.targets\n","X_test = test_dataset.data.float()\n","y_test = test_dataset.targets\n","\n","\n","\n","\n","\n","\n","## Возможно, класс нужно отнаследовать от некого класса из pytorch\n","# class DropoutModel:\n","#    hidden_dim = 512\n","\n","\n","# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","from torch.optim.lr_scheduler import StepLR\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()"],"metadata":{"id":"ylMXA31R0kw6","executionInfo":{"status":"ok","timestamp":1742111915978,"user_tz":-180,"elapsed":21243,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"281c43ed-2469-490e-9884-8a55fe233af6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 26.4M/26.4M [00:01<00:00, 13.8MB/s]\n","100%|██████████| 29.5k/29.5k [00:00<00:00, 229kB/s]\n","100%|██████████| 4.42M/4.42M [00:01<00:00, 4.21MB/s]\n","100%|██████████| 5.15k/5.15k [00:00<00:00, 10.8MB/s]\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision.datasets import FashionMNIST\n","from torchvision.transforms import ToTensor\n","from dataclasses import dataclass\n","\n","torch.manual_seed(987)\n","\n","\n","\n","\n","\n","@dataclass\n","class TrainConfig:\n","    lr: float = 1e-1 # было 1е-3\n","    total_iterations: int = 100\n","\n","\n","# Для оценки будем использовать метрику accuracy\n","# Подумайте (опционально), какие еще метрики можно использовать\n","# на вход функция принимает y_pred и y_true\n","def calculate_accuracy(y_pred: torch.Tensor, y_true: torch.Tensor) -> float:\n","    _, predicted = torch.max(y_pred, 1) # первую переменную(конкретные значения) пропускает, записывате только вторую. после = ищет максимльное значение в y_pred по строчкам (так как 1)(было бы 0 были было по столбцам)\n","    correct = (predicted == y_true).float().sum() # проверка что конкретные значения равны y_true. В тип флоат и суммах их\n","    accuracy = correct / y_true.shape[0]\n","    return accuracy.item()\n","\n","\n","\n","\n","\n","\n","# Загружаем датасет с картинками одежды FashionMNIST\n","# root- путь\n","# train- трейновая часть\n","# download- если отсутствует то загрузить?\n","# transofrm преобразуем сразу в тензор\n","\n","train_dataset = FashionMNIST(\n","    root=\"./data\", train=True, download=True, transform=ToTensor()\n",")\n","test_dataset = FashionMNIST(\n","    root=\"./data\", train=False, download=True, transform=ToTensor()\n",")\n","\n","# тут дело в том, что в датасете есть\n","# интересный момент, в предыущем шаге мы создавали датасет train_dataset с ToTensor\n","# тут мы обращаемся к этому объекту через .data к его неоработанному массиву (не теноры)\n","X_train = train_dataset.data.float()\n","y_train = train_dataset.targets\n","X_test = test_dataset.data.float()\n","y_test = test_dataset.targets\n","\n","\n","\n","\n","\n","\n","## Возможно, класс нужно отнаследовать от некого класса из pytorch\n","# class DropoutModel:\n","#    hidden_dim = 512\n","\n","\n","# штука чтобы показывать прогресс в цикле\n","import tqdm\n","\n","# все методы оптимизации лежат в torch.optim\n","# импортируется стохастический градиентный спуск, который будет обновлять параметры модели\n","import torch.optim\n","#  содержит функциональные интерфейсы для вычисления функций потерь и других операций.\n","import torch.nn.functional as F\n","import torch.nn as nn\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","\n","\n","from torch.optim.lr_scheduler import StepLR\n","\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X_train, y_train, test_size=0.1, random_state=42\n",")\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()\n","\n","\n","\n","\n","\n","torch.manual_seed(987)\n","config = TrainConfig()\n","\n","\n","# Возможно, класс нужно отнаследовать от некого класса из pytorch\n","class ExpModel(nn.Module):\n","  def __init__(self, num_classes: int, dropout_rate: float = 0.3):\n","      super().__init__()\n","\n","\n","      # размерность после первого слоя\n","      hidden_dim = 512\n","\n","      self.net = nn.Sequential(\n","          nn.Linear(in_features= 28*28 , out_features= hidden_dim),\n","          nn.ReLU(),\n","          nn.BatchNorm1d(num_features=hidden_dim),\n","          nn.Dropout(p=dropout_rate),\n","          nn.Linear(in_features=hidden_dim, out_features=num_classes)\n","      )\n","\n","  def forward(self, x: torch.Tensor):\n","        # входной вектор мы решейпим до тензора и этих тензоров столько сколько картинок\n","        # так как это подается в первый слой, он должен иметь такую же размерность\n","        x = x.reshape((-1, 28 * 28))\n","\n","        return self.net(x)\n","\n","\n","\n","  # \"\"\"Обучите здесь модель, подсчитайте метрики на валидационной выборке.\n","\n","  #   Можете так же писать/рисовать accuracy в процессе обучения.\n","  #   Например, каждые 10 итераций или даже каждую итерацию.\n","  #   \"\"\"\n","\n","\n","\n","\n","\n","\n","# model.parameters() возвращает все обучаемые параметры модели (в виде тензоров).\n","# Откуда функция все эти параметры узнает, ведь мы нигде их явно не прописывали в классе модели?\n","# Это магия PyTorch, он умеет многое делать сам :)\n","\n","\n","\n","def train_loop(\n","    model: ExpModel,\n","    X_train: torch.Tensor,\n","    y_train: torch.Tensor,\n","    X_val: torch.Tensor,\n","    y_val: torch.Tensor,\n","    config: TrainConfig,\n","):\n","    # создается оптимайзер\n","    optimizer = optim.SGD(params=model.parameters(), lr= config.lr, momentum=0.9)\n","\n","    losses = []\n","    loss_fn = nn.CrossEntropyLoss()\n","    # Пройдемся 2000 раз по всем данным\n","\n","      #### Новое: создаем разный ШЕДУЛЕР в зависимости от конфига ####\n","    scheduler = StepLR(optimizer, step_size=20, gamma=0.1)\n","\n","\n","\n","\n","\n","    for i in tqdm.trange(config.total_iterations):\n","        # optimizer по умолчанию \"помнит\" градиенты с прошлых итераций - и прибавляет к новым.\n","        # Так сделано ради продвинутых техник обучения.\n","        # Нам это не надо, поэтому в каждой итерации явно зануляем все градиенты - пусть считаются заново.\n","        optimizer.zero_grad()\n","\n","        # Это блок уже знаем: считаем выход, потери, градиенты по потерям.\n","        # Forward pass: вычисляем предсказания модели\n","        # 1 -- ИДЕТ ЗАПУСК МОДЕЛИ ВВЕРХУ!!! Проходит forward pass записывается в аутпут\n","        outputs = model(X_train)\n","\n","        # Вычисляем лосс\n","        loss = loss_fn(outputs, y_train) # Убираем лишние размерности, если они есть\n","\n","        # Backward pass: вычисляем градиенты\n","        loss.backward()\n","\n","        # Просим оптимизатор пройтись по параметрам и сделать градиентный спуск.\n","        # Оптимизатор сам обновит веса, вручную этого делать не надо\n","        optimizer.step()\n","\n","\n","        # Запомним loss\n","        losses.append(loss.detach().item())\n","\n","\n","        # ЦИКЛ ДЛЯ ОЦЕНКИ МОДЕЛИ\n","        if (i + 1) % 10 == 0: # проверяем что i кратен 10 (делится ли на 10 без остатка)\n","            model.eval() # переключаем на режим оценки inference mode\n","            with torch.no_grad(): # отключает подсчеты градиента для все вычислений внутри блока with\n","                train_acc = calculate_accuracy(outputs, y_train) # outputs-предсказания сравнивниваем с y_train\n","                val_outputs = model(X_val) # запускам валидационные данные в модель ?????? Это ещё не вероятности\n","                val_loss = F.cross_entropy(val_outputs, y_val)  #функция потерь. Сравниваем с y_val\n","                val_acc = calculate_accuracy(val_outputs, y_val) # считаем точность\n","            model.train() # включаем обратно BatchNorm и дропауты\n","            print(\n","                f\"Iteration {i+1}: Train Loss = {loss.item():.4f}, Tra  in Acc = {train_acc:.4f}, Val Loss = {val_loss.item():.4f}, Val Acc = {val_acc:.4f}\"\n","            )\n","\n","        scheduler.step()\n","\n","    return losses\n","\n","\n","model = ExpModel(num_classes=10, dropout_rate=0.3)  # Создаём объект!\n","config = TrainConfig(lr=1e-1)  # Создаём объект TrainConfig\n","losses = train_loop(model, X_train, y_train, X_val, y_val, config)\n","torch.save(model.state_dict(), 'model.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-u9UxQRwrYmr","executionInfo":{"status":"ok","timestamp":1742113623659,"user_tz":-180,"elapsed":261798,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"96cf60f5-2e3b-4366-97b9-c6e6b52ab80b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":[" 10%|█         | 10/100 [00:26<03:46,  2.52s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 10: Train Loss = 0.5341, Tra  in Acc = 0.8109, Val Loss = 0.6291, Val Acc = 0.7980\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 20/100 [00:52<03:23,  2.55s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 20: Train Loss = 0.4313, Tra  in Acc = 0.8443, Val Loss = 0.4365, Val Acc = 0.8465\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 30/100 [01:18<03:07,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 30: Train Loss = 0.4168, Tra  in Acc = 0.8507, Val Loss = 0.4015, Val Acc = 0.8548\n"]},{"output_type":"stream","name":"stderr","text":[" 40%|████      | 40/100 [01:45<02:45,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 40: Train Loss = 0.4098, Tra  in Acc = 0.8528, Val Loss = 0.3921, Val Acc = 0.8573\n"]},{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 50/100 [02:11<02:17,  2.75s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 50: Train Loss = 0.4080, Tra  in Acc = 0.8528, Val Loss = 0.3909, Val Acc = 0.8588\n"]},{"output_type":"stream","name":"stderr","text":[" 60%|██████    | 60/100 [02:37<01:50,  2.76s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 60: Train Loss = 0.4060, Tra  in Acc = 0.8538, Val Loss = 0.3902, Val Acc = 0.8592\n"]},{"output_type":"stream","name":"stderr","text":[" 70%|███████   | 70/100 [03:03<01:23,  2.77s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 70: Train Loss = 0.4064, Tra  in Acc = 0.8533, Val Loss = 0.3901, Val Acc = 0.8588\n"]},{"output_type":"stream","name":"stderr","text":[" 80%|████████  | 80/100 [03:29<00:54,  2.74s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 80: Train Loss = 0.4057, Tra  in Acc = 0.8529, Val Loss = 0.3900, Val Acc = 0.8588\n"]},{"output_type":"stream","name":"stderr","text":[" 90%|█████████ | 90/100 [03:55<00:26,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 90: Train Loss = 0.4048, Tra  in Acc = 0.8536, Val Loss = 0.3899, Val Acc = 0.8588\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [04:21<00:00,  2.62s/it]"]},{"output_type":"stream","name":"stdout","text":["Iteration 100: Train Loss = 0.4051, Tra  in Acc = 0.8535, Val Loss = 0.3899, Val Acc = 0.8590\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), 'model.pt')\n","# checkpoint = torch.load('model_501.pt', weights_only=False)\n","# # model.load_state_dict(checkpoint)\n","\n","# from google.colab import files\n","# files.download('model_501.pt')  # Скачиваем файл"],"metadata":{"id":"kylJ3qBq9x2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","files.download('model_501.pt')  # Скачиваем файл"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"QbNL6GXIBvq0","executionInfo":{"status":"ok","timestamp":1742114765705,"user_tz":-180,"elapsed":39,"user":{"displayName":"Bohdan","userId":"11939094435440123406"}},"outputId":"abbf1bf7-2847-40da-c0ef-2380fd98d030"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_20cc61d4-098e-4986-910a-8fdea15ee626\", \"model_501.pt\", 1639830)"]},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y4PqnWBrKcdt"},"outputs":[],"source":["# torch.manual_seed(987)\n","\n","\n","# # Ваш код модели и ее обучения при seed = 987\n","# class ExpModel: ...\n","\n","\n","# model = ExpModel()\n","# config = TrainConfig(...)\n","# train_loop(model, X_train, y_train, X_test, y_test, config)"]},{"cell_type":"markdown","metadata":{"id":"daTen7rUKcdu"},"source":["Наконец, сохраним лучшую модель, чтобы в будущем ее могли взять и использовать, без обучения."]},{"cell_type":"markdown","metadata":{"id":"YvA9w288Kcdu"},"source":["## Задание №9\n","\n","Напишите код, который сохранит модель в файл `model.pt`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKwBP2V4CCJu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Lmsl_0HnBuJH"},"outputs":[],"source":["# Впоследствии эту модель можно будет загрузить вот так:\n","model_loaded = ExpModel(num_classes=len(y_test.unique()))\n","model_loaded.load_state_dict(torch.load(\"model.pt\"))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
